{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import av\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_video_augmentations(video, transform):\n",
    "    targets={'image': video[0]}\n",
    "    for i in range(1, video.shape[0]):\n",
    "        targets[f'image{i}'] = video[i]\n",
    "    transformed = transform(**targets)\n",
    "    transformed = np.concatenate(\n",
    "        [np.expand_dims(transformed['image'], axis=0)] \n",
    "        + [np.expand_dims(transformed[f'image{i}'], axis=0) for i in range(1, video.shape[0])]\n",
    "    )\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = converted_len\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "root_dir = 'UCF-101/UCF-101/'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i for i in os.listdir(root_dir) if i[0] != '.']\n",
    "labels2id = {label:i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77620af283b54d97b186b513f1e46e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = []\n",
    "for label in tqdm(labels):\n",
    "    for video_name in os.listdir(f'{root_dir}/{label}'):\n",
    "        container = av.open(f'{root_dir}/{label}/{video_name}')\n",
    "        if container.streams.video[0].frames > 75:\n",
    "            train.append({\n",
    "                'label': label,\n",
    "                'video_path': f'{root_dir}/{label}/{video_name}'\n",
    "            })\n",
    "train = pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PlayingDhol          164\n",
       "PlayingCello         164\n",
       "HorseRiding          163\n",
       "BoxingPunchingBag    162\n",
       "Drumming             161\n",
       "                    ... \n",
       "BodyWeightSquats      90\n",
       "JavelinThrow          82\n",
       "BlowingCandles        68\n",
       "BasketballDunk        57\n",
       "PushUps               54\n",
       "Name: label, Length: 101, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label_id'] = train.label.map(labels2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, _, _ = train_test_split(train, train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.5, p=0.5)\n",
    "], additional_targets={\n",
    "    f'image{i}': 'image'\n",
    "    for i in range(1, 8)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, meta, transform=None):\n",
    "        self.meta = meta\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "\n",
    "        file_path = self.meta['video_path'].iloc[idx]\n",
    "        container = av.open(file_path)\n",
    "        indices = sample_frame_indices(clip_len=8, frame_sample_rate=5, seg_len=container.streams.video[0].frames)\n",
    "        try:\n",
    "            indices = sample_frame_indices(clip_len=8, frame_sample_rate=5, seg_len=container.streams.video[0].frames)\n",
    "        except Exception:\n",
    "            indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "        if indices.shape[0] < 8:\n",
    "            indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "            \n",
    "        video = read_video_pyav(container, indices)\n",
    "        while video.shape[0] < 8:\n",
    "            video = np.vstack([video, video[-1:]])\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = apply_video_augmentations(video, self.transform)\n",
    "            \n",
    "\n",
    "        inputs = processor(\n",
    "            text=[''],\n",
    "            videos=list(video),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        for i in inputs:\n",
    "            inputs[i] = inputs[i][0]\n",
    "\n",
    "        return inputs, self.meta['label_id'].iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ActionDataset(meta=X_train, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=15)\n",
    "\n",
    "val_dataset = ActionDataset(meta=X_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=101, bias=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "model.to(device)\n",
    "classifier = nn.Linear(512, len(labels))\n",
    "classifier.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen XClip training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "freeze_epochs = 5\n",
    "model_lr = 1e-5\n",
    "classifier_lr = 1e-3\n",
    "\n",
    "model_optimizer = optim.AdamW(model.parameters(), model_lr)\n",
    "classifier_optimizer = torch.optim.AdamW(classifier.parameters(), lr=classifier_lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33c329615a046f9865a7401cdaf3b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 0:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.748993673422231\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58dd5e027a20435c97e20fb9b5059075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 0:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.9145336138958835\n",
      "F1: 0.8244219464627169\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf242472ae2d452b89c6a332ab38e5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 1:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.2512337396576134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4847c09c35104c939c3096261ac3f093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 1:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.690573743411473\n",
      "F1: 0.8721783842761593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83155f37faf144d19102bde7c4ee3cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 2:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.3135608704220312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f973015c8314f8d993085509515b29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 2:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.028557603456536\n",
      "F1: 0.9075624404063354\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ea65beefda4a8a9bcbb364706d0056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 3:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8378346111790719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62a2918f15e491b9902f02a7f44d7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 3:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.7018090010601647\n",
      "F1: 0.9284045562234075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62af47739e1349838c8baa8b66c439d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 4:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5966130399459865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4ec52683ea4132b283e0c2cc56fbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 4:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5270408716584954\n",
      "F1: 0.935476840782045\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    model.eval()\n",
    "    classifier.train()     \n",
    "\n",
    "    train_loss = []\n",
    "    for i, (batch, targets) in enumerate(tqdm(train_dataloader, desc=f\"Epoch: {epoch}\")):\n",
    "        classifier_optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        logits = classifier(outputs.video_embeds)\n",
    "\n",
    "        loss = criterion(logits, targets) \n",
    "        loss.backward()\n",
    "        classifier_optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    print('Training loss:', np.mean(train_loss))\n",
    "\n",
    "    model.eval() \n",
    "    classifier.eval()    \n",
    "\n",
    "    val_loss = []\n",
    "    val_targets = []\n",
    "    val_preds = []\n",
    "    for i, (batch, targets) in enumerate(tqdm(val_dataloader, desc=f\"Epoch: {epoch}\")):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            logits = classifier(outputs.video_embeds)\n",
    "\n",
    "            loss = criterion(logits, targets) \n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            val_targets.extend(targets.cpu().numpy())\n",
    "            val_preds.extend(logits.argmax(axis=1).cpu().numpy())\n",
    "\n",
    "    print('Val loss:', np.mean(val_loss))\n",
    "    print('F1:', f1_score(val_targets, val_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full XClip training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.text_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ecc5f779224f1bbcbdefdc9f99ed80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 0:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.27012621462599407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804bb8e5854a4ef0a54e881dfa784199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 0:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1985938476816732\n",
      "F1: 0.9514175167728979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685be5f8159a47d9895e811cc77ec3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 1:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.06677514677326936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442aa93b08dc4a81938c0382234c6cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 1:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.13393537319094248\n",
      "F1: 0.964959740182343\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594655145cf041b7b26d1849313a422b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 2:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.03211416319050785\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc386baf7fe4840922cf0e20d290987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 2:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.08779511839740586\n",
      "F1: 0.9757769098558563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e95adca9a34d8da2775119cbdd4415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 3:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0136671471234678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c04e34248a492da59bb9024f486d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 3:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.07810192471085002\n",
      "F1: 0.9786765262877234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052516e306e64951b39d76729dbb108e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 4:   0%|          | 0/586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.10124871646508625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72501b80ae04f249d4454ce7c378850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 4:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1280776294642033\n",
      "F1: 0.965067701623843\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train() \n",
    "    classifier.train()     \n",
    "\n",
    "    train_loss = []\n",
    "    for i, (batch, targets) in enumerate(tqdm(train_dataloader, desc=f\"Epoch: {epoch}\")):\n",
    "        model_optimizer.zero_grad()\n",
    "        classifier_optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        logits = classifier(outputs.video_embeds)\n",
    "\n",
    "        loss = criterion(logits, targets) \n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "        classifier_optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    print('Training loss:', np.mean(train_loss))\n",
    "\n",
    "    model.eval()\n",
    "    classifier.eval()   \n",
    "\n",
    "    val_loss = []\n",
    "    val_targets = []\n",
    "    val_preds = []\n",
    "    for i, (batch, targets) in enumerate(tqdm(val_dataloader, desc=f\"Epoch: {epoch}\")):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            logits = classifier(outputs.video_embeds)\n",
    "\n",
    "            loss = criterion(logits, targets) \n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            val_targets.extend(targets.cpu().numpy())\n",
    "            val_preds.extend(logits.argmax(axis=1).cpu().numpy())           \n",
    "\n",
    "    print('Val loss:', np.mean(val_loss))\n",
    "    print('F1:', f1_score(val_targets, val_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
